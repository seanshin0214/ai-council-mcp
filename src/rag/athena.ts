import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { pool } from '../db.js';
import { getCachedResponse, setCachedResponse } from '../redis.js';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
const googleAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY || '');

// Perplexity API (OpenAI í˜¸í™˜)
const perplexity = new OpenAI({
  apiKey: process.env.PERPLEXITY_API_KEY || '',
  baseURL: 'https://api.perplexity.ai',
});

interface QueryOptions {
  model?: 'claude' | 'gpt4' | 'gemini' | 'o1' | 'gemini-pro' | 'perplexity';
  topK?: number;
  useCache?: boolean;
}

// RAG ì¿¼ë¦¬ ì²˜ë¦¬ - Athena
export async function queryAthena(query: string, options: QueryOptions = {}) {
  const {
    model = 'gpt4', // GPT-5ë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì • (Claude API í‚¤ ì—†ì„ ê²½ìš° ëŒ€ë¹„)
    topK = 5,
    useCache = true,
  } = options;

  try {
    // 1. ì„ì‹œ ë¬¸ì„œ(user_answers) ìš°ì„  í™•ì¸ - ìœ ì‚¬ë„ ê²€ìƒ‰ ì—†ì´ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
    console.error('ğŸ” Checking for temporary user-provided documents...');
    const tempDocsResult = await pool.query(
      `SELECT content, metadata FROM documents
       WHERE metadata->>'type' = 'user_answers'
       AND metadata->>'temporary' = 'true'
       ORDER BY created_at DESC
       LIMIT $1`,
      [topK]
    );

    if (tempDocsResult.rows.length > 0) {
      console.error(`âœ… Found ${tempDocsResult.rows.length} temporary user documents - using directly without similarity search`);
      const relevantDocs = tempDocsResult.rows;

      // ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
      const context = relevantDocs
        .map((doc, idx) => `[${idx + 1}] ${doc.content}`)
        .join('\n\n');

      // AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
      const answer = await generateAnswer(query, context, model);

      return {
        answer,
        sources: relevantDocs.map(doc => ({
          content: doc.content.substring(0, 200) + '...',
          metadata: doc.metadata,
        })),
        fromCache: false,
      };
    }

    console.error('â„¹ï¸ No temporary documents found, falling back to similarity search...');

    // 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    const queryEmbedding = await createEmbedding(query);

    // 3. ì‹œë§¨í‹± ìºì‹± í™•ì¸
    if (useCache) {
      const cached = await getCachedResponse(queryEmbedding);
      if (cached) {
        console.error('Cache hit - returning cached response');
        return {
          answer: cached,
          sources: [],
          fromCache: true,
        };
      }
    }

    // 4. ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    console.error(`ğŸ” Searching for similar documents (topK=${topK})...`);
    const relevantDocs = await searchSimilarDocuments(queryEmbedding, topK);
    console.error(`ğŸ” Found ${relevantDocs.length} relevant documents`);

    if (relevantDocs.length === 0) {
      // ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸
      const countResult = await pool.query('SELECT COUNT(*) as total FROM documents');
      const totalDocs = parseInt(countResult.rows[0]?.total || '0');
      console.error(`âš ï¸ No similar documents found. Total documents in DB: ${totalDocs}`);

      return {
        answer: `No relevant documents found in the knowledge base. (Total documents in DB: ${totalDocs})`,
        sources: [],
        fromCache: false,
      };
    }

    // 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const context = relevantDocs
      .map((doc, idx) => `[${idx + 1}] ${doc.content}`)
      .join('\n\n');

    // 5. AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„± (ì‹¤íŒ¨ ì‹œ í´ë°±)
    let answer: string;
    try {
      answer = await generateAnswer(query, context, model);
    } catch (error) {
      console.error(`Model ${model} failed, falling back to GPT-5:`, error);
      // í´ë°±: GPT-5ë¡œ ì¬ì‹œë„
      if (model !== 'gpt4') {
        try {
          answer = await generateAnswer(query, context, 'gpt4');
        } catch (fallbackError) {
          console.error('Fallback to GPT-5 also failed:', fallbackError);
          throw new Error('All AI models failed to generate answer');
        }
      } else {
        throw error;
      }
    }

    // 6. ìºì‹±
    if (useCache) {
      await setCachedResponse(queryEmbedding, answer);
    }

    return {
      answer,
      sources: relevantDocs.map(doc => ({
        content: doc.content.substring(0, 200) + '...',
        metadata: doc.metadata,
      })),
      fromCache: false,
    };
  } catch (error) {
    console.error('Athena query error:', error);
    throw error;
  }
}

// ì„ë² ë”© ìƒì„±
async function createEmbedding(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  const firstData = response.data[0];
  return firstData?.embedding || [];
}

// ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
async function searchSimilarDocuments(embedding: number[], topK: number) {
  const result = await pool.query(
    `SELECT content, metadata,
            1 - (embedding <=> $1::vector) as similarity
     FROM documents
     ORDER BY embedding <=> $1::vector
     LIMIT $2`,
    [`[${embedding.join(',')}]`, topK]
  );

  return result.rows;
}

// AI ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
async function generateAnswer(
  query: string,
  context: string,
  model: 'claude' | 'gpt4' | 'gemini' | 'o1' | 'gemini-pro' | 'perplexity'
): Promise<string> {
  const systemPrompt = `You are Athena, an AI assistant with access to a knowledge base.
Answer the user's question based on the provided context.
If the context doesn't contain relevant information, say so clearly.

Context:
${context}`;

  switch (model) {
    case 'claude': {
      const response = await anthropic.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1024,
        messages: [
          {
            role: 'user',
            content: `${systemPrompt}\n\nQuestion: ${query}`,
          },
        ],
      });

      const firstContent = response.content[0];
      return firstContent && firstContent.type === 'text' ? firstContent.text : '';
    }

    case 'gpt4': {
      const response = await openai.chat.completions.create({
        model: 'gpt-4o', // GPT-4oë¡œ ë³€ê²½ (gpt-5ëŠ” ì•„ì§ ì§€ì› ì•ˆë¨)
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: query },
        ],
        max_completion_tokens: 8192, // max_tokens -> max_completion_tokens
        temperature: 0.7,
      });

      const firstChoice = response.choices[0];
      return firstChoice?.message?.content || '';
    }

    case 'gemini': {
      const gemini = googleAI.getGenerativeModel({
        model: 'gemini-2.0-flash-exp', // Gemini 2.0 Flash (1.5 ProëŠ” 2025ë…„ í˜„ì¬ deprecated)
        generationConfig: {
          maxOutputTokens: 8192,
          temperature: 0.7,
        },
      });
      const response = await gemini.generateContent(`${systemPrompt}\n\nQuestion: ${query}`);
      return response.response.text();
    }

    case 'o1': {
      // o1-preview: ìµœê³  ì¶”ë¡  ëŠ¥ë ¥
      const response = await openai.chat.completions.create({
        model: 'o1-preview',
        messages: [
          { role: 'user', content: `${systemPrompt}\n\nQuestion: ${query}` },
        ],
      });

      const firstChoice = response.choices[0];
      return firstChoice?.message?.content || '';
    }

    case 'gemini-pro': {
      // Gemini Pro: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
      const gemini = googleAI.getGenerativeModel({
        model: 'gemini-2.0-flash-exp', // Gemini 2.0 Flash (1.5 ProëŠ” deprecated)
        generationConfig: {
          maxOutputTokens: 8192,
          temperature: 0.7,
        },
      });
      const response = await gemini.generateContent(`${systemPrompt}\n\nQuestion: ${query}`);
      return response.response.text();
    }

    case 'perplexity': {
      // Perplexity Sonar Pro: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ + ìµœì‹  ì •ë³´
      const response = await perplexity.chat.completions.create({
        model: 'sonar-pro', // Perplexity Pro ëª¨ë¸
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: query },
        ],
        max_tokens: 4096,
        temperature: 0.7,
      } as any); // Perplexity íŠ¹í™” íŒŒë¼ë¯¸í„°ëŠ” ëŸ°íƒ€ì„ì— ì²˜ë¦¬

      const firstChoice = response.choices[0];
      return firstChoice?.message?.content || '';
    }

    default:
      throw new Error(`Unknown model: ${model}`);
  }
}
