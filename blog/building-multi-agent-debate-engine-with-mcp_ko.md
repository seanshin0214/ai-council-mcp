# MCP로 멀티에이전트 토론 엔진 구축하기
## 논문을 읽기 전에 Generate-Debate-Evolve를 독자적으로 구현한 이야기

*게시일: 2024년 12월*

---

> **3줄 요약**
> - MCP(Model Context Protocol) 위에 "3라운드 멀티모델 토론 엔진"을 구현했다.
> - 핵심은 프롬프트가 아니라 오케스트레이션(비용/지연/로그/안전) 엔지니어링이었다.
> - 나중에 "Generate–Debate–Evolve"류의 연구를 읽으며, 이미 같은 방향을 실사용으로 구현하고 있었다는 걸 깨달았다.

> **Disclaimer**
> 어떤 연구소를 능가했다는 주장이 아니다.
> "비슷한 개념을 독립적으로 먼저 구현해서 쓰고 있었다"는 뜻이다.
> 본 프로젝트는 Google DeepMind 및 어떤 모델 제공사와도 무관하다.

---

## 1. 문제: 단일 모델 답변은 '빠르지만 취약'하다

연구/개발 현장에서 단일 LLM 답변은 종종:
- 자신감 있게 틀리고,
- 중요한 반례를 놓치고,
- 왜 그런 결론이 나왔는지 검증하기가 어렵다.

그래서 내 결론은 단순했다.

> "한 모델이 답하면 위험하다.
> 여러 모델이 서로 반박하게 만들자."

---

## 2. 구현: Generate → Debate → Evolve (3라운드)

내가 구현한 프로토콜은 간단하지만 룰이 엄격하다.

### Round 1 — Generate (Divergence)
각 모델이 서로의 답을 보지 못한 채 독립적으로 답한다.
목표는 '정답'이 아니라 **다양한 관점/가설 확보**다.

### Round 2 — Debate (Cross-examination)
각 모델은 다른 모델의 답을 보고 반드시:
- 숨은 가정 지적
- 반례 제시
- 대안/개선안 제안
- 불확실성과 추가 검증 항목 표시
를 수행한다.

### Round 3 — Evolve (Convergence)
"Chair" 모델(Claude)이 비판과 근거를 종합해:
- 최종 결론
- 남은 리스크
- 다음 검증 체크리스트
까지 포함한 결과물을 만든다.

> 핵심은 "환각 제거" 같은 과장이 아니라
> "오류 위험을 줄이고, 검증 가능한 형태로 만들었다"는 점이다.

---

## 3. 왜 MCP로 만들었나: 챗봇이 아니라 '엔진'이 필요했다

MCP로 만들면:
- Claude Desktop/IDE/Agent 런타임 어디서든 호출 가능
- debate를 "tool"로 재사용 가능
- RAG/코드생성/워크플로 도구들과 쉽게 결합 가능

즉, "기능"이 아니라 **"인프라"**가 된다.

### 구현한 도구들

| 도구 | 용도 |
|------|------|
| `ask_council_questions` | 토론 전 40개 구조화 질문 생성 |
| `start_council_discussion` | 3라운드 토론 실행 |
| `query_knowledge_base` | RAG 검색 + 자동 모델 선택 |
| `analyze_query_complexity` | 적절한 모델로 라우팅 |
| `add_document` | 지식베이스에 문서 적재 |

---

## 4. 진짜 어려웠던 건 엔지니어링 (4가지)

### (1) 비용
멀티모델 = 비용 증가.
그래서 정책을 넣었다:
- **Submarine Mode**: 비선택 모델은 대기 상태 (33-67% 절감)
- 쉬운 질문은 라운드/모델 축소
- 유사 질문은 캐싱
- 복잡한 문제에만 풀 토론 실행

### (2) 지연(Latency)
토론은 느리다.
병렬 호출 + 스트리밍 합성으로 체감 지연을 낮췄다.

현재 벤치마크:
| 작업 | 응답 시간 |
|------|----------|
| 단일 모델 쿼리 | 0.8-2초 |
| 전체 Council (3라운드, 4모델) | 15-25초 |

### (3) 감사 가능성(Auditability)
"왜 그런 결론이 나왔나?"를 위해:
- 라운드별 입력/출력
- 각 모델의 비판 내용
- 합성 근거
를 남긴다.

### (4) 안전/권한
MCP 도구는 데이터와 시스템에 접근할 수 있다.
그래서:
- 최소권한
- 상태 변경 도구는 사용자 확인
- 로그 정제/레이트리밋
을 기본 원칙으로 한다.

---

## 5. 최소 오케스트레이션 스케치

```
INPUT: user_query

Round1 = [Model_i.generate(user_query) for i in models]  // 병렬
Round2 = [Model_i.critique(Round1, policy) for i in models]  // 병렬
Final  = Chair.synthesize(user_query, Round1, Round2, policy)

OUTPUT: Final (+ traces)
```

실제 구현은 다음을 처리한다:
- API 실패 및 재시도
- 토큰 카운팅 및 비용 추적
- 모델별 페르소나 주입
- RAG 컨텍스트 조립

---

## 6. "진짜 좋아졌나?" 측정 없이는 주장하지 않는다

신뢰성 향상을 말하려면 평가가 필요하다.

### 평가 프레임워크

**고정 질문 세트** (50~200개)

**블라인드 사람 평가** (단일모델 vs Council)

**평가 기준:**
- 정확성
- 불확실성 처리
- 근거 사용(RAG 사용 시)
- 실행가능성

**시스템 메트릭:**
- p50/p95 지연
- 쿼리당 비용
- 캐시 히트율

### 초기 결과

| 지표 | 단일 모델 | Council |
|------|----------|---------|
| 정확성 (사람 평가) | ~85% | ~93% |
| 불확실성 표시 | 드묾 | 대부분 |
| 쿼리당 비용 | $0.02 | $0.08-0.13 |

*참고: 내부 관찰 결과이며, 엄격한 벤치마크는 아님.*

---

## 7. 다음 단계

패턴 이름이 아니라 다음이 중요하다:

1. **비동기 실행** (더 강한 병렬화 + 스트리밍 UX)
2. **토너먼트형 진화** (복수 토론 후보 중 최적 선택)
3. **Judge Loop** (critique scoring + 인간 spot-check)
4. **도메인 스택 결합** (연구 방법론, 코드패턴, 재현성 번들)

---

## 8. 배운 점

### 잘 된 것
- 모델끼리 비판하게 하면 내가 놓쳤을 오류를 잡아냄
- Submarine Mode 덕분에 경제적으로 운용 가능
- MCP 패키징으로 여러 프로젝트에서 재사용 가능

### 안 된 것
- 초기 버전은 매번 4개 모델 전부 참여 → 비용 과다
- 동기식 라운드 실행은 너무 느림 → 병렬 호출로 개선
- 범용 프롬프트는 약한 비판 생성 → 도메인별 비판 템플릿으로 품질 향상

### 놀라웠던 점
- 모델들은 생성보다 비판을 더 잘함
- Perplexity의 웹 검색이 다른 모델이 놓친 반례를 자주 찾아냄
- Claude가 최종 합성자로서 GPT-4o보다 더 잘 작동함

---

## 링크

- **저장소**: [github.com/seanshin0214/ai-council-mcp](https://github.com/seanshin0214/ai-council-mcp)
- **MCP Spec**: [modelcontextprotocol.io/specification](https://modelcontextprotocol.io/specification/)
- **참고 논문**: [Towards an AI co-scientist (arXiv)](https://arxiv.org/abs/2502.18864)

---

## 마무리

패턴은 누구나 말할 수 있다.
하지만 "엔진"은 비용/지연/로그/안전/재현성을 통과해야 한다.

나는 그걸 먼저 만들었고,
논문은 나중에 읽었다.

---

*질문이나 피드백? [GitHub 이슈](https://github.com/seanshin0214/ai-council-mcp/issues)에 남겨주세요.*
